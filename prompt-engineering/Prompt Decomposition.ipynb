{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c5ad2e-7475-48e1-a34c-127e28bb3674",
   "metadata": {},
   "source": [
    "# Prompt Decomposition\n",
    "This notebook contains an example of prompt decomposition, or taking one long prompt and breaking it down into smaller parts.  These smaller parts can then be run independantly, often in parallel, and often on smaller models.  This can lead to significant performance enhancements and cost savings, may increase quality, and will make your prompts much easier to maintain as the workload grows.  This is because each step can be maintained and tested independantly, without any tweaks on one step impacting all the others (which may occur if they're all one huge prompt).\n",
    "\n",
    "Here are three common times when Prompt Decomposition can be helpful:\n",
    "  1) Preprocessing of RAG or context data.  For example, if context data is large, such a a long support document, consider a prompt that summarizes that content once, then future queries retrieve the summary rather than the long document. \n",
    "  2)  Breaking multi-step prompts into a maintainable DAG.  This can be helpful when a prompt reads like code, with a large number of steps or if/then instructions.  Instead, consider breaking these out and generating a flow diagram, resulting in maintainable individual pieces.\n",
    "  3)  Streamlining long linear prompts.  Even where prompts have only a single logical flow, if they are long, it can help to break the flow into small, sequential steps.  These steps combined may execute faster than the long original prompt, and they can also be maintained and tested independently. \n",
    "\n",
    "The notebook follows this structure:\n",
    "  1) Set up the envionment\n",
    "  2) Examples of decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93e1be-d464-4f6c-8019-31e15a5450a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1) Set up the envionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "de410f15-afe9-4600-845d-343fbf68b7d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anthropic\n",
      "  Downloading anthropic-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from anthropic) (3.5.0)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from anthropic)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from anthropic) (1.10.13)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from anthropic) (1.2.0)\n",
      "Collecting tokenizers>=0.13.0 (from anthropic)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting typing-extensions<5,>=4.7 (from anthropic)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic) (3.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->anthropic)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Collecting huggingface_hub<1.0,>=0.16.4 (from tokenizers>=0.13.0->anthropic)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.6.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages/PyYAML-6.0-py3.10-linux-x86_64.egg (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.1.0)\n",
      "Downloading anthropic-0.21.1-py3-none-any.whl (851 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m851.2/851.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m640.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m991.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, httpcore, fsspec, distro, huggingface_hub, httpx, tokenizers, anthropic\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.4 which is incompatible.\n",
      "panel 0.13.1 requires bokeh<2.5.0,>=2.4.0, but you have bokeh 3.3.2 which is incompatible.\n",
      "spyder 5.3.3 requires ipython<8.0.0,>=7.31.1, but you have ipython 8.18.1 which is incompatible.\n",
      "spyder 5.3.3 requires pylint<3.0,>=2.5.0, but you have pylint 3.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed anthropic-0.21.1 distro-1.9.0 fsspec-2024.3.1 httpcore-1.0.4 httpx-0.27.0 huggingface_hub-0.21.4 tokenizers-0.15.2 typing-extensions-4.10.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "735e1180-1b93-49e2-b9c5-445938069561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use Anthropics library only to count tokens locally\n",
    "from anthropic import Anthropic\n",
    "client = Anthropic()\n",
    "def count_tokens(text):\n",
    "    return client.count_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d4e65081-5f5b-4153-97b8-333df3be2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for connecting with Bedrock, use Boto3\n",
    "import boto3, time, json\n",
    "from botocore.config import Config\n",
    "\n",
    "#increase the standard time out limits in boto3, because Bedrock may take a while to respond to large requests.\n",
    "my_config = Config(\n",
    "    connect_timeout=60*3,\n",
    "    read_timeout=60*3,\n",
    ")\n",
    "bedrock = boto3.client(service_name='bedrock-runtime',config=my_config)\n",
    "bedrock_service = boto3.client(service_name='bedrock',config=my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6675b967-5e61-48f0-bced-2faba4fa89eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claud-v3 found!\n"
     ]
    }
   ],
   "source": [
    "#check that it's working:\n",
    "models = bedrock_service.list_foundation_models()\n",
    "for line in models[\"modelSummaries\"]:\n",
    "    #print (line[\"modelId\"])\n",
    "    pass\n",
    "if \"anthropic.claude-3\" in str(models):\n",
    "    print(\"Claud-v3 found!\")\n",
    "else:\n",
    "    print (\"Error, no model found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0b3a2e45-43b6-4ac4-8240-5dc3b58ca220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_ATTEMPTS = 1 #how many times to retry if Claude is not working.\n",
    "session_cache = {} #for this session, do not repeat the same query to claude.\n",
    "def ask_claude(messages,system=\"\", DEBUG=False, model=\"haiku\"):\n",
    "    '''\n",
    "    Send a prompt to Bedrock, and return the response.  Debug is used to see exactly what is being sent to and from Bedrock.\n",
    "    messages can be an array of role/message pairs, or a string.\n",
    "    '''\n",
    "    raw_prompt_text = str(messages)\n",
    "    \n",
    "    if type(messages)==str:\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "    \n",
    "    promt_json = {\n",
    "        \"system\":system,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 10000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"anthropic_version\":\"\",\n",
    "        \"top_k\": 250,\n",
    "        \"top_p\": 0.7,\n",
    "        \"stop_sequences\": [\"\\n\\nHuman:\"]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    if DEBUG: print(\"sending:\\nSystem:\\n\",system,\"\\nMessages:\\n\",\"\\n\".join(messages))\n",
    "    \n",
    "    if model== \"opus\":\n",
    "        modelId = 'error'\n",
    "    elif model== \"sonnet\":\n",
    "        modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    elif model== \"haiku\":\n",
    "        modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "    else:\n",
    "        print (\"ERROR:  Bad model, must be opus, sonnet, or haiku.\")\n",
    "        modelId = 'error'\n",
    "    \n",
    "    if raw_prompt_text in session_cache:\n",
    "        return [raw_prompt_text,session_cache[raw_prompt_text]]\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        try:\n",
    "            response = bedrock.invoke_model(body=json.dumps(promt_json), modelId=modelId, accept='application/json', contentType='application/json')\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            results = response_body.get(\"content\")[0].get(\"text\")\n",
    "            if DEBUG:print(\"Recieved:\",results)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Error with calling Bedrock: \"+str(e))\n",
    "            attempt+=1\n",
    "            if attempt>MAX_ATTEMPTS:\n",
    "                print(\"Max attempts reached!\")\n",
    "                results = str(e)\n",
    "                break\n",
    "            else:#retry in 10 seconds\n",
    "                time.sleep(10)\n",
    "    session_cache[raw_prompt_text] = results\n",
    "    return [raw_prompt_text,results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f002e219-1bf7-4190-9b5e-0ca5acb28fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please say the number four.\n",
      "Four.\n",
      "CPU times: user 5.2 ms, sys: 235 µs, total: 5.44 ms\n",
      "Wall time: 355 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#check that it's working:\n",
    "try:\n",
    "    query = \"Please say the number four.\"\n",
    "    #query = [{\"role\": \"user\", \"content\": \"Please say the number two.\"},{\"role\": \"assistant\", \"content\": \"Two.\"},{\"role\": \"user\", \"content\": \"Please say the number three.\"}]\n",
    "    result = ask_claude(query)\n",
    "    print(query)\n",
    "    print(result[1])\n",
    "except Exception as e:\n",
    "    print(\"Error with calling Claude: \"+str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "36d36a19-d377-474d-a046-4584b313c60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# Threaded function for queue processing.\n",
    "def thread_request(q, result):\n",
    "    while not q.empty():\n",
    "        work = q.get()                      #fetch new work from the Queue\n",
    "        thread_start_time = time.time()\n",
    "        try:\n",
    "            data = ask_claude(work[1],model=work[2])\n",
    "            result[work[0]] = data          #Store data back at correct index\n",
    "        except Exception as e:\n",
    "            error_time = time.time()\n",
    "            print('Error with prompt!',str(e))\n",
    "            result[work[0]] = (str(e))\n",
    "        #signal to the queue that task has been processed\n",
    "        q.task_done()\n",
    "    return True\n",
    "\n",
    "def ask_claude_threaded(prompts,model=\"haiku\",DEBUG=False):\n",
    "    '''\n",
    "    Call ask_claude, but multi-threaded.\n",
    "    Returns a dict of the prompts and responces.\n",
    "    '''\n",
    "    q = Queue(maxsize=0)\n",
    "    num_theads = min(50, len(prompts))\n",
    "    \n",
    "    #Populating Queue with tasks\n",
    "    results = [{} for x in prompts];\n",
    "    #load up the queue with the promts to fetch and the index for each job (as a tuple):\n",
    "    for i in range(len(prompts)):\n",
    "        #need the index and the url in each queue item.\n",
    "        q.put((i,prompts[i],model))\n",
    "        \n",
    "    #Starting worker threads on queue processing\n",
    "    for i in range(num_theads):\n",
    "        #print('Starting thread ', i)\n",
    "        worker = Thread(target=thread_request, args=(q,results))\n",
    "        worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to \n",
    "                                  #exit eventually even if these dont finish \n",
    "                                  #correctly.\n",
    "        worker.start()\n",
    "\n",
    "    #now we wait until the queue has been processed\n",
    "    q.join()\n",
    "\n",
    "    if DEBUG:print('All tasks completed.')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ecc70b33-ec10-49e3-8d5f-0cdcde538bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118/3064970181.py:39: DeprecationWarning: setDaemon() is deprecated, set the daemon attribute instead\n",
      "  worker.setDaemon(True)    #setting threads as \"daemon\" allows main program to\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Please say the number one.', 'One.'], ['Please say the number two.', 'two'], ['Please say the number three.', 'three'], ['Please say the number four.', 'Four.'], ['Please say the number five.', 'Five.']]\n",
      "CPU times: user 48.7 ms, sys: 19 ms, total: 67.7 ms\n",
      "Wall time: 580 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#test if our threaded Claude calls are working\n",
    "q1 = [{\"role\": \"user\", \"content\": \"Please say the number one.\"}]\n",
    "q2 = [{\"role\": \"user\", \"content\": \"Please say the number two.\"}]\n",
    "q3 = [{\"role\": \"user\", \"content\": \"Please say the number three.\"}]\n",
    "#print(ask_claude_threaded([q1,q2,q3]))\n",
    "print(ask_claude_threaded([\"Please say the number one.\",\"Please say the number two.\",
\"Please say the number three.\",\"Please say the number four.\",
\"Please say the number five.\"],model='sonnet'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0cb22-d6b7-44e9-b451-5dd11cea93fd",
   "metadata": {},
   "source": [
    "## 2) Examples of decomposition\n",
    "\n",
    "Here we'll consider an example use case of a user who would like to undersand what is going on in their area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f434a-070a-4f3c-8619-acbf4fc1e478",
   "metadata": {},
   "source": [
    "### Start by downloading some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "79880dfd-b644-465b-b545-08c990e19d9e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found articles on BBC front page.  Downloading full text...\n",
      "https://www.bbc.com/news/world-us-canada-68613083\n",
      "https://www.bbc.com/news/world-europe-68616372\n",
      "https://www.bbc.com/news/world-europe-guernsey-68615288\n",
      "https://www.bbc.com/news/world-australia-68572280\n",
      "https://www.bbc.com/news/business-68619144\n",
      "https://www.bbc.com/news/world-us-canada-68621004\n",
      "https://www.bbc.com/news/world-us-canada-68609687\n",
      "https://www.bbc.com/news/world-us-canada-68486307\n",
      "https://www.bbc.com/news/world-africa-68606201\n",
      "https://www.bbc.com/news/world-middle-east-68614549\n",
      "https://www.bbc.com/news/world-europe-68618722\n",
      "https://www.bbc.com/news/world-us-canada-68616476\n",
      "https://www.bbc.com/news/world-us-canada-68621005\n",
      "https://www.bbc.com/news/uk-scotland-68610426\n",
      "https://www.bbc.com/news/world-us-canada-68618342\n",
      "https://www.bbc.com/news/uk-68609361\n",
      "https://www.bbc.com/news/world-68614782\n",
      "https://www.bbc.com/news/world-asia-china-68508694\n",
      "https://www.bbc.com/news/world-europe-40134140\n",
      "https://www.bbc.com/news/world-us-canada-68609685\n",
      "https://www.bbc.com/news/uk-scotland-68610426\n",
      "https://www.bbc.com/news/uk-england-12457929\n",
      "https://www.bbc.com/news/uk-20624370\n",
      "https://www.bbc.com/news/world-us-canada-68570993\n",
      "https://www.bbc.com/news/world-68615052\n",
      "https://www.bbc.com/news/articles/cz7z6xnpyn6o\n",
      "https://www.bbc.com/news/world-africa-68615717\n",
      "Max tokens reached, ending download.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests, re\n",
    "from bs4 import BeautifulSoup \n",
    "  \n",
    "url = 'https://www.bbc.com/news'\n",
    "response = requests.get(url) \n",
    "  \n",
    "soup = BeautifulSoup(response.text, 'html.parser') \n",
    "#print (soup)\n",
    "headlines = soup.find('body').find_all(\"a\",{\"data-testid\" : \"internal-link\"}) \n",
    "\n",
    "def get_bbc_text(url:str) -> list:\n",
    "    \"\"\"Parse bbc article and return text in list of strings\"\"\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "\n",
    "    text = re.findall( r'{\"text\":\"(.*?)\",\"',response.text)\n",
    "    #print(response.text)\n",
    "    full_text = \"\"\n",
    "    last_one = \"\"\n",
    "    for t in text:\n",
    "        if len(t)<3:continue\n",
    "        if t==last_one:\n",
    "            continue\n",
    "        last_one = t\n",
    "        full_text += \"\\n\" + t\n",
    "    return full_text\n",
    "\n",
    "articles = []\n",
    "MAX_TOKENS = 30000\n",
    "current_token_count = 0\n",
    "print (\"Found articles on BBC front page.  Downloading full text...\")\n",
    "for x in list(dict.fromkeys(headlines)): \n",
    "    if len(x.text.strip())>30: #skip categories, only look at headlines\n",
    "        title = x.text.strip()\n",
    "        url2 = 'https://www.bbc.com' + x[\"href\"]\n",
    "        print (url2)\n",
    "        text = get_bbc_text(url2)\n",
    "        articles.append(text)\n",
    "        current_token_count += count_tokens(text)\n",
    "        if current_token_count>MAX_TOKENS:\n",
    "            print (\"Max tokens reached, ending download.\")\n",
    "            break\n",
    "        \n",
    "print (\"Done.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "68f4ccaa-5922-49fb-a78c-85499714e347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles: 27\n",
      "Total length 137979\n",
      "Length in tokens: 30479\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of articles:\",len(articles))\n",
    "print (\"Total length\",sum(len(x) for x in articles))\n",
    "print (\"Length in tokens:\", count_tokens(\"\\n\".join(articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4861bda7-6792-4e3d-9611-bf6f610329d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found articles on San Jose Mercury News front page.  Downloading full text...\n",
      "Max tokens reached, ending download.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.mercurynews.com/'\n",
    "response = requests.get(url) \n",
    "  \n",
    "soup = BeautifulSoup(response.text, 'html.parser') \n",
    "#print (soup)\n",
    "headlines = soup.find('body').find_all(\"a\") \n",
    "\n",
    "def get_san_jose_text(url:str) -> list:\n",
    "    \"\"\"Parse bbc article and return text in list of strings\"\"\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    #print (url)\n",
    "    #print (response.text)\n",
    "    local_soup = BeautifulSoup(response.text, 'html.parser') \n",
    "    text = local_soup.find('body').getText()\n",
    "    text = text.split('\\n')\n",
    "    full_text = []\n",
    "    for t in text:\n",
    "        t = t.strip()\n",
    "        if len(t)<60:continue\n",
    "        full_text.append(t)\n",
    "    full_text = '\\n'.join(full_text)\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "local_articles = []\n",
    "MAX_TOKENS = 30000\n",
    "current_token_count = 0\n",
    "print (\"Found articles on San Jose Mercury News front page.  Downloading full text...\")\n",
    "for x in list(dict.fromkeys(headlines)): \n",
    "    if len(x.text.strip())>30: #skip categories, only look at headlines\n",
    "        title = x.text.strip()\n",
    "        if title in [\"Do Not Sell/Share My Personal Information\"]:continue\n",
    "        url2 = x['href']\n",
    "        text = get_san_jose_text(url2)\n",
    "        local_articles.append(text)\n",
    "        current_token_count += count_tokens(text)\n",
    "        if current_token_count>MAX_TOKENS:\n",
    "            print (\"Max tokens reached, ending download.\")\n",
    "            break\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9d1e9899-35a3-4780-811d-01f469449cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of local articles: 18\n",
      "Total length 135067\n",
      "Length in tokens: 31698\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of local articles:\",len(local_articles))\n",
    "print (\"Total length\",sum(len(x) for x in local_articles))\n",
    "print (\"Length in tokens:\", count_tokens(\"\\n\".join(local_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e8518fc8-561b-4141-a990-528f18f34b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "long_prompt_template = \"\"\"Consider the following information:\n",
    "<local_news>{{LOCAL_NEWS}}</local_news>\n",
    "<global_news>{{GLOBAL_NEWS}}</global_news>\n",
    "\n",
    "Please answer this question: \n",
    "<question>{{QUESTION}}</question>\n",
    "\"\"\"\n",
    "\n",
    "short_prompt_template = \"\"\"Consider the following news articles:\n",
    "<news>{{NEWS}}</news>\n",
    "These articles have been scraped from an HTML page, and may contain extra text scraps that are safe to ignore.\n",
    "Please summarize key points in each article in brief concise language that preserves important details, and write the key points of each article in its own <article> tag.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b3a8bb79-1fe6-451a-be65-9ae18c60517d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prompt length in tokens: 62492\n"
     ]
    }
   ],
   "source": [
    "local_news_prompt = \"<article>\" + \"</article>\\n<article>\".join(local_articles) + \"</article>\"\n",
    "global_news_prompt = \"<article>\" + \"</article>\\n<article>\".join(articles) + \"</article>\"\n",
    "\n",
    "question = \"Are there any people, places, or ideas mentioned in both the local and global news?\"\n",
    "\n",
    "test_long_prompt = long_prompt_template.replace(\"{{LOCAL_NEWS}}\",local_news_prompt).replace(\"{{GLOBAL_NEWS}}\",global_news_prompt).replace(\"{{QUESTION}}\",question)\n",
    "\n",
    "print (\"Total prompt length in tokens:\",count_tokens(test_long_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bf7d679c-9a1a-46b9-b8eb-9fd7bf3a3d81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there are a few people, places, and ideas mentioned in both the local and global news sections:\n",
      "\n",
      "People:\n",
      "- Princess Anne - There is an article in the local news about an attempted kidnapping of Princess Anne in 1974, and a video in the global news section about her police bodyguard who was shot while thwarting the kidnap attempt.\n",
      "\n",
      "Places: \n",
      "- Gaza - There are articles in both sections discussing the ongoing conflict and humanitarian crisis in Gaza.\n",
      "- Sudan - Both sections have articles covering the civil war and famine risk in Sudan.\n",
      "\n",
      "Ideas:\n",
      "- Immigration laws/policies - There are articles in both sections discussing controversial immigration laws, such as the strict new law proposed in Texas (SB4) and Hong Kong's new national security law (Article 23) that critics say erodes civil liberties.\n",
      "\n",
      "So while the specific articles differ, there is some overlap in the people, places, and broad topics/ideas covered in the local Bay Area news and global international news sections.\n",
      "CPU times: user 7.93 ms, sys: 1.96 ms, total: 9.9 ms\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "long_responce = ask_claude(test_long_prompt, model=\"sonnet\")[1]\n",
    "print(long_responce)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe75e7-432c-4764-bef0-46ed14dd979c",
   "metadata": {},
   "source": [
    "## Not bad!  60K tokens processed in about 30 seconds.  Let's see if we can make that faster and cheaper using prompt decoposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f29a01a5-0f19-4fbe-a5db-a86607d3e329",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local tokens: 31868\n",
      "global tokens: 30708\n"
     ]
    }
   ],
   "source": [
    "test_short_prompt_local = short_prompt_template.replace(\"{{NEWS}}\",local_news_prompt)\n",
    "test_short_prompt_global = short_prompt_template.replace(\"{{NEWS}}\",global_news_prompt)\n",
    "\n",
    "print (\"local tokens:\",count_tokens(test_short_prompt_local))\n",
    "print (\"global tokens:\",count_tokens(test_short_prompt_global))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f2ca0d24-c013-4af4-a647-7ac0e426886b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<article>\n",
      "Key points:\n",
      "- A company operating 10 nursing homes in the Bay Area has settled a lawsuit by local county prosecutors and the State of California alleging it neglected vulnerable patients' me ...\n",
      "CPU times: user 10.2 ms, sys: 469 µs, total: 10.6 ms\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "local_responce = ask_claude(test_short_prompt_local, model=\"haiku\")[1]\n",
    "print (local_responce[:200],\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ac8aec32-4a8a-4af4-92fd-a460b7b7be33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<article>\n",
      "Key points:\n",
      "- A controversial Texas law known as SB4 would allow local and state police to arrest and prosecute undocumented migrants, upending federal immigration enforcement.\n",
      "- The law is  ...\n",
      "CPU times: user 7.3 ms, sys: 170 µs, total: 7.47 ms\n",
      "Wall time: 35.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "global_responce = ask_claude(test_short_prompt_global, model=\"haiku\")[1]\n",
    "print (global_responce[:200],\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "466bf053-4df6-46f7-9b7e-8665d0e798b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prompt length in tokens: 4132\n"
     ]
    }
   ],
   "source": [
    "local_news_prompt = local_responce\n",
    "global_news_prompt = global_responce\n",
    "\n",
    "question = \"Are there any people, places, or ideas mentioned in both the local and global news?\"\n",
    "\n",
    "test_long_prompt_decomposed = long_prompt_template.replace(\"{{LOCAL_NEWS}}\",local_news_prompt).replace(\"{{GLOBAL_NEWS}}\",global_news_prompt).replace(\"{{QUESTION}}\",question)\n",
    "\n",
    "print (\"Total prompt length in tokens:\",count_tokens(test_long_prompt_decomposed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ac8c2817-f25d-471c-b358-a189ad5682b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the news articles provided, there does not appear to be any direct overlap of specific people, places, or ideas mentioned in both the local Bay Area news and the global news stories. The local news focuses on events and issues within the Bay Area region, while the global news covers a diverse range of international stories from various countries around the world.\n",
      "CPU times: user 4.04 ms, sys: 2.13 ms, total: 6.18 ms\n",
      "Wall time: 3.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "decomposed_response = ask_claude(test_long_prompt_decomposed, model=\"sonnet\")[1]\n",
    "print (decomposed_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f59d2ec8-cc80-4545-a358-16ce6f1954fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there are a few people, places, or ideas mentioned in both the local and global news articles:\n",
      "\n",
      "1. Famine/Hunger Crisis: The global news article mentions the severe humanitarian crisis and threat of famine in Gaza, as well as the potential for the conflict in Sudan to trigger the world's largest hunger crisis. This relates to the local news article that discusses the need for \"single payer\" healthcare legislation to curb profiteering in the healthcare system.\n",
      "\n",
      "2. Discrimination/Exclusion: The global news article discusses the lawsuit against the \"Ladies Lounge\" exhibit in Australia for excluding men, which relates to the local news article about the nursing home company that allegedly neglected vulnerable patients and exposed them to physical and sexual assaults.\n",
      "\n",
      "3. Police Misconduct/Civil Rights Violations: The global news article mentions the sentencing of former Mississippi police officers for torturing two black men, which relates to the local news article about the arrest of a man suspected of installing a hidden camera in a Starbucks women's restroom and possessing numerous firearms.\n",
      "\n",
      "So in summary, the themes of healthcare, discrimination/exclusion, and police misconduct/civil rights violations are present in both the local and global news articles.\n",
      "CPU times: user 0 ns, sys: 5.9 ms, total: 5.9 ms\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session_cache = {}#don't use cached info, since we. want to time this.\n",
    "decomposed_response = ask_claude(test_long_prompt_decomposed, model=\"haiku\")[1]\n",
    "print (decomposed_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6217efd-f29c-403f-ba6a-355a5c747608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
